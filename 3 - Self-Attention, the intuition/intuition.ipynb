{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Self-Attention, Backborn of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention without trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Take an simple example of 6 words sentence\n",
    "\n",
    "#### sentence is: How self attention works in Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3178, -0.8048,  0.7659],\n",
       "        [ 0.6087,  0.6313,  0.4881],\n",
       "        [-0.5776,  0.5329,  1.2401],\n",
       "        [-0.7568,  1.0494, -0.4434],\n",
       "        [ 1.8883, -1.5365, -0.5007],\n",
       "        [-1.2233, -0.2219, -1.2253]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each word has word embeddings lenght of 3\n",
    "# Let's create a random word embedding\n",
    "\n",
    "word_embeddings = torch.randn(6, 3)\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6087, 0.6313, 0.4881])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row in embeddings repersentence a word from example sentence in same sequence\n",
    "# Let's try to understand how self attention works\n",
    "\n",
    "# Calulating attention for word \"self\" in example sentence\n",
    "word_embed_self = word_embeddings[1]\n",
    "word_embed_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3276,  1.0072,  0.5901, -0.0147, -0.0648, -1.4828])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will calculate simalarity score against each word embedding in same sentence\n",
    "# Let's calculate simalarity score \n",
    "sim_score = torch.tensor([torch.dot(word_embed_self,embed) for embed in word_embeddings])\n",
    "sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1195, -3.4418, -2.0162,  0.0502,  0.2215,  5.0668])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the intuition that word 'attention' has the higest simalarity score.\n",
    "# However, the word embedding are choosen randomly, just for illestration.\n",
    "\n",
    "# Now let's calculate attention weights by normalizing the score\n",
    "# there are many ways to normalize the score, like division with sum of all values.\n",
    "sim_weights  = sim_score/sim_score.sum()\n",
    "sim_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0972, 0.3694, 0.2434, 0.1329, 0.1264, 0.0306])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is not looking good, it will diviate the model training weights while backprop\n",
    "# We can use softmax for torch\n",
    "sim_weights = torch.softmax(sim_score, axis=-1)\n",
    "sim_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax converts the score to a range between 0 and 1, that sum to 1. (like probability)\n",
    "sim_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1540, 0.2231, 0.3968])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To generate context vector for word 'self', we need to calculate weighted sum of all words embeddings\n",
    "# using for loop to keep the process more intutuive, instead of matrix multiplication\n",
    "\n",
    "context_vector = torch.zeros(3)\n",
    "for i in range(6):\n",
    "    context_vector += sim_weights[i] * word_embeddings[i]\n",
    "context_vector\n",
    "# As we can see, context vector for 'self' is a weighted sum of all word embeddings in sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3354, -0.3276,  0.7045, -0.9436,  0.2530, -0.3711],\n",
       "        [-0.3276,  1.0072,  0.5901, -0.0147, -0.0648, -1.4828],\n",
       "        [ 0.7045,  0.5901,  2.1554,  0.4466, -2.5304, -0.9311],\n",
       "        [-0.9436, -0.0147,  0.4466,  1.8706, -2.8194,  1.2363],\n",
       "        [ 0.2530, -0.0648, -2.5304, -2.8194,  6.1770, -1.3556],\n",
       "        [-0.3711, -1.4828, -0.9311,  1.2363, -1.3556,  3.0472]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's create context vector for all words against each word in sentence\n",
    "similarty_mat = torch.matmul(word_embeddings, word_embeddings.T)    # word_embeddings @ word_embeddings.T\n",
    "similarty_mat\n",
    "\n",
    "# a 6x6 matrix, simalrity for each word in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.2655e-01, 8.0862e-02, 2.2698e-01, 4.3673e-02, 1.4451e-01, 7.7419e-02],\n",
       "        [9.7219e-02, 3.6939e-01, 2.4339e-01, 1.3294e-01, 1.2644e-01, 3.0624e-02],\n",
       "        [1.3955e-01, 1.2446e-01, 5.9548e-01, 1.0782e-01, 5.4933e-03, 2.7190e-02],\n",
       "        [3.0096e-02, 7.6201e-02, 1.2086e-01, 5.0202e-01, 4.6118e-03, 2.6622e-01],\n",
       "        [2.6600e-03, 1.9358e-03, 1.6447e-04, 1.2318e-04, 9.9458e-01, 5.3247e-04],\n",
       "        [2.6468e-02, 8.7083e-03, 1.5120e-02, 1.3207e-01, 9.8898e-03, 8.0774e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization\n",
    "similarty_weights = torch.softmax(similarty_mat, axis=-1)   # axis for each word in sentence\n",
    "similarty_weights\n",
    "\n",
    "# we can comapre 'self' similarty weigths comapring with previously computed\n",
    "# tensor([0.0972, 0.3694, 0.2434, 0.1329, 0.1264, 0.0306])                      -> previously computed\n",
    "# [9.7219e-02, 3.6939e-01, 2.4339e-01, 1.3294e-01, 1.2644e-01, 3.0624e-02],     -> above computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarty_weights[0].sum(axis=-1)       # just to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0723, -0.3647,  0.4611],\n",
       "        [ 0.1540,  0.2231,  0.3968],\n",
       "        [-0.4170,  0.3823,  0.8222],\n",
       "        [-0.7299,  0.5489, -0.3410],\n",
       "        [ 1.8775, -1.5290, -0.4955],\n",
       "        [-1.0813, -0.0636, -1.0100]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context matrix for each word in sentence\n",
    "\n",
    "context_matrix = torch.matmul(similarty_weights, word_embeddings)\n",
    "context_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: \n",
    "    # The words embeddings are randomly chosen, might not make sense while genreating similarty_weights\n",
    "    # In real implementation, we would use positional embedding along with word embedding,\n",
    "    # but in this example, we just used word embedding.\n",
    "    # Also, in real implementation, we would use matrix multiplication for word embedding and positional embedding,\n",
    "    # instead of for loop.\n",
    "    # Also, in real implementation, we would use softmax for attention weights, instead of matrix multiplication.\n",
    "    \n",
    "    # This tutorial have single attention head for illestration purposes\n",
    "    # In real implementation, we would have multiple attention heads, and each head would have its own weights and bias.\n",
    "    # And we would also use scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
