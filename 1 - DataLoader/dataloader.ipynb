{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code for PyTorch Dataloader in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data for GPT like model training\n",
    "# using same text which we used in tokenizer\n",
    "\n",
    "sample_text = '''Lexical tokenization is conversion of a text into meaningful lexical tokens belonging to categories defined by a \"lexer\" program. \n",
    "In case of a natural language, those categories include nouns, verbs, adjectives, punctuations etc. \n",
    "In case of a programming language, the categories include identifiers, operators, grouping symbols and data types. \n",
    "Lexical tokenization is related to the type of tokenization used in large language models but with two differences. \n",
    "First, lexical tokenization is usually based on a lexical grammar, whereas LLM tokenizers are usually probability-based. \n",
    "Second, LLM tokenizers perform a second step that converts the tokens into numerical values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lexical tokenization is conversion of a text into meaningful lexical tokens belonging to categories defined by a \"lexer\" program. ',\n",
       " 'In case of a natural language, those categories include nouns, verbs, adjectives, punctuations etc. ',\n",
       " 'In case of a programming language, the categories include identifiers, operators, grouping symbols and data types. ',\n",
       " 'Lexical tokenization is related to the type of tokenization used in large language models but with two differences. ',\n",
       " 'First, lexical tokenization is usually based on a lexical grammar, whereas LLM tokenizers are usually probability-based. ',\n",
       " 'Second, LLM tokenizers perform a second step that converts the tokens into numerical values']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: write preprocessing setps for your data\n",
    "\n",
    "\n",
    "# no processing needed for this sample data\n",
    "processed_text = sample_text.split('\\n') if isinstance(sample_text, str) else sample_text\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer from previous, tokenizer.ipynb\n",
    "class simpleTokenizer:\n",
    "    def __init__(self, text=None, cased=True):\n",
    "        self.cased = cased\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        if text is not None:\n",
    "            self.train(text)\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        if self.cased: text = text.lower()\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        return text\n",
    "        \n",
    "    def train(self, text):\n",
    "        print('Training Tokenizer...')\n",
    "        vocab = set()\n",
    "        text = self.preprocess(text)\n",
    "        lines = text.split('\\n')\n",
    "        print(f'Total Sentences: %s' % len(lines))\n",
    "        \n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            for token in tokens:\n",
    "                vocab.add(token)\n",
    "        \n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<unk>')\n",
    "        print(f'Vocab size: %s' % len(vocab))\n",
    "        \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "        self.idx_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.preprocess(text)\n",
    "        tokens = text.split()\n",
    "        return [self.token_to_idx[token] if token in self.token_to_idx else self.token_to_idx['<unk>'] for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        return ' '.join([self.idx_to_token[idx] if idx in self.idx_to_token else '<unk>' for idx in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tokenizer...\n",
      "Total Sentences: 1\n",
      "Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "tokenizer = simpleTokenizer(' '.join(processed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 53, 24, 11, 35, 1, 48, 23, 30, 28, 55, 6, 52, 10, 14, 8, 1, 0, 40],\n",
       " [21, 9, 35, 1, 32, 26, 51, 10, 22, 33, 62, 2, 42, 16],\n",
       " [21, 9, 35, 1, 41, 26, 50, 10, 22, 20, 37, 19, 47, 3, 13, 58],\n",
       " [28, 53, 24, 43, 52, 50, 57, 35, 53, 59, 21, 27, 25, 31, 7, 64, 56, 15],\n",
       " [17, 28, 53, 24, 60, 5, 36, 1, 28, 18, 63, 29, 54, 4, 60, 39],\n",
       " [45, 29, 54, 38, 1, 44, 46, 49, 12, 50, 55, 23, 34, 61]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize text\n",
    "token_seq = [ tokenizer.encode(line) for line in processed_text]\n",
    "token_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT like model used to predict next word, \n",
    "# so we will prepare data that have next word in prediction to calculate loss\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, x, tokenizer, max_len, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_seq = [ tokenizer.encode(line) for line in x ]\n",
    "        \n",
    "        for token_ids in token_seq:\n",
    "            for i in range(0, len(token_ids)-max_len, stride):\n",
    "                self.input_ids.append(torch.tensor(token_ids[i:i+max_len]))\n",
    "                self.target_ids.append(torch.tensor(token_ids[i+1:i+max_len+1]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPTDataset(processed_text, tokenizer=tokenizer, max_len=4, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([28, 53, 24, 11]), tensor([53, 24, 11, 35]), 73)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.input_ids[0], dataset.target_ids[0], dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch style dataloader\n",
    "\n",
    "def create_dataloader(input_text, batch_size = 2, max_len = 4, stride = 1, shuffle = False, drop_last=True, num_workers = 0):\n",
    "    dataset = GPTDataset(input_text, tokenizer=tokenizer, max_len=max_len, stride=stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[28, 53, 24, 11],\n",
      "        [53, 24, 11, 35]]), tensor([[53, 24, 11, 35],\n",
      "        [24, 11, 35,  1]])]\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
