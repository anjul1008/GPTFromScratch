{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copmact and optimized Casual Single Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffers in NN\n",
    "- PyTorch process everything in Tensors\n",
    "- NN Modules used to put in device with single statement but on torch.nn modules not any other tensor\n",
    "- mask tensor is not the part of nn module in atention mechanism.\n",
    "- either we have to manually put in device(GPU) or use Buffer to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base code of selfAttentionImproved used from Single Head Self-Attention\n",
    "\n",
    "class compacCasultSHSL(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout=0.5, qkv_biased=False):\n",
    "        super(compacCasultSHSL, self).__init__()\n",
    "        self.d_out = d_out\n",
    "        self.liner_query = nn.Linear(d_in, d_out, bias=qkv_biased)   # default value of requires_grad is True\n",
    "        self.liner_key = nn.Linear(d_in, d_out, bias=qkv_biased)\n",
    "        self.liner_value = nn.Linear(d_in, d_out, bias=qkv_biased)\n",
    "        self.droput = nn.Dropout(0.5)       # 50% probability of dropout, GPT model 0.1 or 0.2 is used\n",
    "        \n",
    "        # # for CPU it works fine, but when we put model into GPU \"mask\" doesn't follow the same\n",
    "        # # and keeps running on CPU only, which leads to error\n",
    "        # self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "        # model itself follows the device\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.liner_query(x)\n",
    "        keys = self.liner_key(x)\n",
    "        values = self.liner_value(x)\n",
    "        \n",
    "        # Compute attention score\n",
    "        att_score = queries @ keys.transpose(1,2)\n",
    "        \n",
    "        # MASKING future attention score, replace with inf that will be changed to zero by softmat\n",
    "        # fill by -inf where it finds Ture\n",
    "        att_score.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        \n",
    "        # att_score = att_score.masked_fill(self.mask.bool(), -torch.inf)\n",
    "        print('att_score', att_score)\n",
    "        \n",
    "        # attention weight\n",
    "        norm_factor = keys.shape[-1] ** 0.5              # normalization factor    \n",
    "        att_weights = torch.softmax(att_score/norm_factor, dim=-1)\n",
    "        print(f\"Masked att_weights marix: {att_weights}\")\n",
    "        \n",
    "        # Appling droput to masked att_weights\n",
    "        droput_att_weights = self.droput(att_weights)\n",
    "        print(f\"Dropout att_weights marix: {droput_att_weights}\")\n",
    "        \n",
    "        # context matrix\n",
    "        context = droput_att_weights @ values\n",
    "        \n",
    "        return context\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1115,  0.1204, -0.3696],\n",
       "         [-0.2404, -1.1969,  0.2093],\n",
       "         [-0.9724, -0.7550,  0.3239],\n",
       "         [-0.1085,  0.2103, -0.3908]],\n",
       "\n",
       "        [[-0.1115,  0.1204, -0.3696],\n",
       "         [-0.2404, -1.1969,  0.2093],\n",
       "         [-0.9724, -0.7550,  0.3239],\n",
       "         [-0.1085,  0.2103, -0.3908]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input\n",
    "torch.manual_seed(123)\n",
    "inputs = torch.randn(4,3)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_length 4\n",
      "W_query.device: cpu\n",
      "mask.device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "compacCasultSHSL(\n",
       "  (liner_query): Linear(in_features=3, out_features=3, bias=False)\n",
       "  (liner_key): Linear(in_features=3, out_features=3, bias=False)\n",
       "  (liner_value): Linear(in_features=3, out_features=3, bias=False)\n",
       "  (droput): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = batch.shape[1]\n",
    "print('context_length', context_length)\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 3\n",
    "ccsa = compacCasultSHSL(d_in=d_in, d_out=d_out, context_length=context_length)\n",
    "print(\"W_query.device:\", ccsa.liner_query.weight.device)\n",
    "print(\"mask.device:\", ccsa.mask.device)\n",
    "ccsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_score tensor([[[ 0.0356,    -inf,    -inf,    -inf],\n",
      "         [-0.1919, -0.0616,    -inf,    -inf],\n",
      "         [-0.2273, -0.2666, -0.4173,    -inf],\n",
      "         [ 0.0489, -0.0534,  0.0238,  0.0548]],\n",
      "\n",
      "        [[ 0.0356,    -inf,    -inf,    -inf],\n",
      "         [-0.1919, -0.0616,    -inf,    -inf],\n",
      "         [-0.2273, -0.2666, -0.4173,    -inf],\n",
      "         [ 0.0489, -0.0534,  0.0238,  0.0548]]])\n",
      "Masked att_weights marix: tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4812, 0.5188, 0.0000, 0.0000],\n",
      "         [0.3480, 0.3402, 0.3118, 0.0000],\n",
      "         [0.2543, 0.2398, 0.2507, 0.2552]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4812, 0.5188, 0.0000, 0.0000],\n",
      "         [0.3480, 0.3402, 0.3118, 0.0000],\n",
      "         [0.2543, 0.2398, 0.2507, 0.2552]]])\n",
      "Dropout att_weights marix: tensor([[[2.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9624, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6960, 0.6804, 0.0000, 0.0000],\n",
      "         [0.5087, 0.4795, 0.5014, 0.5104]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9624, 1.0376, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5087, 0.0000, 0.5014, 0.5104]]])\n",
      "Context Matrix after Self-Attention: tensor([[[ 0.1770, -0.0929, -0.1812],\n",
      "         [ 0.0852, -0.0447, -0.0872],\n",
      "         [-0.3817,  0.0509, -0.4055],\n",
      "         [-0.3856,  0.0462, -0.6701]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.5909,  0.0823, -0.6095],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.0731, -0.0125, -0.4287]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context = ccsa.forward(batch)\n",
    "print(f\"Context Matrix after Self-Attention: {context}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
