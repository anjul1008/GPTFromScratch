{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a simple Tokenizer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Example for Tokenizer, which don't include indepth preprocessing.\n",
    "'''\n",
    "\n",
    "class simpleTokenizer:\n",
    "    def __init__(self, text=None, cased=True):\n",
    "        self.cased = cased\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        if text is not None:\n",
    "            self.train(text)\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        if self.cased: text = text.lower()\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        return text\n",
    "        \n",
    "    def train(self, text):\n",
    "        print('Training Tokenizer...')\n",
    "        vocab = set()\n",
    "        text = self.preprocess(text)\n",
    "        lines = text.split('\\n')\n",
    "        print(f'Total Sentences: %s' % len(lines))\n",
    "        \n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            for token in tokens:\n",
    "                vocab.add(token)\n",
    "        \n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<unk>')\n",
    "        print(f'Vocab size: %s' % len(vocab))\n",
    "        \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "        self.idx_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.preprocess(text)\n",
    "        tokens = text.split()\n",
    "        return [self.token_to_idx[token] if token in self.token_to_idx else self.token_to_idx['<unk>'] for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        return ' '.join([self.idx_to_token[idx] if idx in self.idx_to_token else '<unk>' for idx in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''\n",
    "Lexical tokenization is conversion of a text into meaningful lexical tokens belonging to categories defined by a \"lexer\" program. \n",
    "In case of a natural language, those categories include nouns, verbs, adjectives, punctuations etc. \n",
    "In case of a programming language, the categories include identifiers, operators, grouping symbols and data types. \n",
    "Lexical tokenization is related to the type of tokenization used in large language models but with two differences. \n",
    "First, lexical tokenization is usually based on a lexical grammar, whereas LLM tokenizers are usually probability-based. \n",
    "Second, LLM tokenizers perform a second step that converts the tokens into numerical values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tokenizer...\n",
      "Total Sentences: 8\n",
      "Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = simpleTokenizerCased()\n",
    "# tokenizer.train(sample_text)\n",
    "tokenizer = simpleTokenizer(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 29, 54, 38, 1, 44, 46, 49, 12, 50, 55, 23, 34, 61]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode('Second, LLM tokenizers perform a second step that converts the tokens into numerical values')\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'second, llm tokenizers perform a second step that converts the tokens into numerical values'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
