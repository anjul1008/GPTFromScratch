{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention in Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer architecture follows the same intuition but in a silightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will follow \"Attention all you need\" paper to illestimate self attention mechanism.\n",
    "# In transformer architecture:\n",
    "    # it apply attention  using 3 different weight matrix: key, query and value matrix\n",
    "    # instead of using just one matrix or weights with input matrix, transformer architecture follows below algorithm\n",
    "        # context_vector_i = (key_matrix * (query_matrix * x_i))/constant_value * value_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2191, -0.9015, -0.2318],\n",
       "        [ 0.5199, -0.9474, -0.0275],\n",
       "        [-0.5163, -0.0144,  0.2551],\n",
       "        [ 0.2500,  0.3028,  0.6914],\n",
       "        [-1.6208,  0.7710,  0.3911]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's understand it with example\n",
    "# Example sentence: \"Our LLM journey starts here\"\n",
    "\n",
    "# input embedding matrix\n",
    "x = torch.randn(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gred = False    # keeping gredients false to keep output cluster free, True for training\n",
    "d_in = d_out = 3    # keeping 3x3 matrix of all three matrix\n",
    "q = torch.nn.Parameter(torch.rand(3,3), requires_grad=use_gred)         # trainable parameter, while training\n",
    "k = torch.nn.Parameter(torch.rand(3,3), requires_grad=use_gred)         # trainable parameter, while training\n",
    "v = torch.nn.Parameter(torch.rand(3,3), requires_grad=use_gred)         # trainable parameter, while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5213, -0.6311,  0.0208]),\n",
       " tensor([ 0.3267, -0.1724, -0.0928]),\n",
       " tensor([0.1461, 0.1484, 0.1638]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating attention for word \"LLM\"\n",
    "x_1 = x[1]  # word embedding of word \"LLM\" \n",
    "\n",
    "q_x1 = x_1 @ q\n",
    "k_x1 = x_1 @ k\n",
    "v_x1 = x_1 @ v \n",
    "\n",
    "q_x1, k_x1, v_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0635)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnormalized attention score for word \"LLM\", it's word \"LLM\" attention to \"LLM\"\n",
    "# \"LLM\" -> \"LLM\" attention\n",
    "attn_score_11 = q_x1.dot(k_x1)\n",
    "attn_score_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2838, -0.0635,  0.3444, -0.6775,  0.9437])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generalizing the computation to all attention\n",
    "# attention score againest all words sentence\n",
    "keys = (x @ k)\n",
    "attn_score_1 = q_x1 @ keys.T\n",
    "attn_score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2044, 0.1673, 0.2117, 0.1174, 0.2992])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention weights for word \"LLM\"\n",
    "constant_vaule = keys.shape[-1] ** 0.5\n",
    "attn_weight_1 = torch.softmax(attn_score_1 / constant_vaule, dim=-1)\n",
    "attn_weight_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3535, -0.2929, -0.0785])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context vector for word \"LLM\" after attention\n",
    "# \"LLM\" context vector\n",
    "values = x @ v\n",
    "context_vector_1 = attn_weight_1 @ values\n",
    "context_vector_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all words attention againest all words in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_all = x @ q\n",
    "k_all = x @ k\n",
    "v_all = x @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6778e-01, -9.7939e-02,  4.8369e-01, -1.1508e+00,  1.3818e+00],\n",
       "        [ 2.8379e-01, -6.3466e-02,  3.4444e-01, -6.7752e-01,  9.4370e-01],\n",
       "        [ 2.5976e-02,  1.7880e-02, -1.0293e-03, -2.9809e-02, -1.1422e-02],\n",
       "        [-5.9335e-01,  1.2894e-01, -5.1991e-01,  1.5237e+00, -1.5767e+00],\n",
       "        [-6.3557e-02,  6.4684e-02, -1.9389e-01,  1.9309e-01, -5.1222e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention score\n",
    "att_score = q_all @ k_all.T\n",
    "att_score\n",
    "\n",
    "# you can mathch (1,1) index value with previously compute attention score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2075, 0.1497, 0.2095, 0.0815, 0.3518],\n",
       "        [0.2044, 0.1673, 0.2117, 0.1174, 0.2992],\n",
       "        [0.2030, 0.2020, 0.1998, 0.1965, 0.1986],\n",
       "        [0.1329, 0.2017, 0.1387, 0.4513, 0.0753],\n",
       "        [0.2026, 0.2182, 0.1879, 0.2350, 0.1564]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization score\n",
    "atten_wights = torch.softmax(att_score / k_all.shape[-1] ** 0.5, dim=-1)\n",
    "atten_wights, atten_wights[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4343, -0.3608, -0.1297],\n",
       "        [-0.3535, -0.2929, -0.0785],\n",
       "        [-0.1872, -0.1532,  0.0273],\n",
       "        [ 0.1347,  0.1094,  0.2661],\n",
       "        [-0.1114, -0.0897,  0.0759]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context matrix after attention\n",
    "contex_matrix = atten_wights @ v_all\n",
    "contex_matrix\n",
    "\n",
    "# we can check 1st word context matches with previously compute attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
