{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Multi Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSAOptimized(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, n_heads=2, causal=True, dropout=0.1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_heads == 0), 'd_out must be a multiple of n_heads'\n",
    "        # self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.causal = causal\n",
    "        # self.dropout = dropout\n",
    "        self.head_dim = d_out // n_heads\n",
    "        \n",
    "        # k, q, v\n",
    "        self.query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # liner projection, not a nesseary projection, can be skipped\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if self.causal:\n",
    "            self.register_buffer(\n",
    "                \"mask\",\n",
    "                torch.triu(torch.ones(context_len, context_len),\n",
    "                        diagonal=1)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        b, n_tokens, d_in = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # reshaping for n_heads attention, and transpose\n",
    "        # (b, n_tokens, self.n_heads, self.head_dim) -> (b, self.n_heads, n_tokens, self.head_dim)\n",
    "        q = q.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        att_score = q @ k.transpose(2, 3) # out -> (b, self.n_heads, n_tokens, n_tokens)\n",
    "        \n",
    "        # MASKING future attention score, replace with inf that will be changed to zero by softmax\n",
    "        if self.causal:\n",
    "            att_score.masked_fill_(self.mask.bool()[:n_tokens, :n_tokens], float('-inf'))\n",
    "        att_weights = torch.softmax(att_score/k.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        # dropout\n",
    "        att_weights = self.dropout(att_weights)\n",
    "        \n",
    "        # context vectors\n",
    "        # -> (b, self.n_heads, n_tokens, self.head_dim) -> (b, n_tokens, self.n_heads, self.head_dim)\n",
    "        context_vec = (att_score @ v).transpose(1, 2)\n",
    "        # print(context_vec)   \n",
    "        \n",
    "        context_vec = context_vec.contiguous().view(b, n_tokens, self.d_out)\n",
    "        \n",
    "        # liner projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2922,  1.5814],\n",
       "          [ 0.9303,  0.6592],\n",
       "          [ 0.3796, -0.3670]],\n",
       " \n",
       "         [[ 0.2922,  1.5814],\n",
       "          [ 0.9303,  0.6592],\n",
       "          [ 0.3796, -0.3670]]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input\n",
    "torch.manual_seed(124)\n",
    "inputs = torch.randn(3,2)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_length 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MHSAOptimized(\n",
       "  (query): Linear(in_features=2, out_features=2, bias=False)\n",
       "  (key): Linear(in_features=2, out_features=2, bias=False)\n",
       "  (value): Linear(in_features=2, out_features=2, bias=False)\n",
       "  (out_proj): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_len = batch.shape[1]\n",
    "print('context_length', context_len)\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "# causal will lead to nan and -inf with small dimension inputs\n",
    "mhsa = MHSAOptimized(d_in=d_in, d_out=d_out, context_len=context_len, n_heads=2, \n",
    "                    causal=False, dropout=0.2, qkv_bias=False)\n",
    "mhsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Matrix after multi head Self-Attention:\n",
      " tensor([[[ 0.9573, -0.0071],\n",
      "         [ 0.9248, -0.0426],\n",
      "         [ 0.4983, -0.5417]],\n",
      "\n",
      "        [[ 0.9573, -0.0071],\n",
      "         [ 0.9248, -0.0426],\n",
      "         [ 0.4983, -0.5417]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context = mhsa.forward(batch)\n",
    "print(f\"Context Matrix after multi head Self-Attention:\\n\", context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
