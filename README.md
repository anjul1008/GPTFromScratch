# LearnGPTFromScratchWithAnjul

This repository is a step-by-step guide to understanding and implementing GPT-like models from scratch. Each section focuses on a key component, starting from data loading to reconstructing a simplified GPT-2 model. If you want to dive deep into transformers and self-attention mechanisms, this repository is for you!

## ðŸ“Œ Topics Covered

1. **DataLoader**  
   Implementation of a simple data loader to prepare text data for training.

2. **Word Embeddings and Positional Encoding**  
   Understanding token embeddings and how positional information is incorporated.

3. **Self-Attention, the Intuition**  
   Conceptual explanation of self-attention and why it is crucial for transformers.

4. **Self-Attention in Transformers**  
   Detailed breakdown of self-attention in transformer models.

5. **Single Head Self-Attention**  
   Implementation of a single-head self-attention mechanism.

6. **Causal Self-Attention**  
   Introduction to causal self-attention, which ensures information flows from left to right.

7. **Compact Causal Self-Attention with Buffer**  
   Optimized implementation of causal self-attention using buffers.

8. **Multi-Head Self-Attention**  
   Implementation of multi-head self-attention, a key component of transformers.

9. **Reconstructing GPT-2 Model**  
   Combining all components to build a simplified GPT-2 model from scratch.
